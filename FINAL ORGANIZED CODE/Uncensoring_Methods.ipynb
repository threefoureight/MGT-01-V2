{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdfd6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1447e",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11222f",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a69a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_baseline_uncensoring(df):\n",
    "    df_result = df.copy()\n",
    "    df_result['Baseline_Demand'] = df_result['Verkauf']\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01287916",
   "metadata": {},
   "source": [
    "## Naive Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed537889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_n1_uncensoring(df):\n",
    "    \"\"\"\n",
    "    N1: replace censored values with mean of all values within each POS.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['N1_Demand'] = df['Verkauf'].copy()\n",
    "\n",
    "    # Identify closed observations\n",
    "    is_closed = (df[\"Zensiert\"] == 1)\n",
    "\n",
    "    # Compute mean Verkauf per group and broadcast using transform\n",
    "    group_means = df.groupby(['EHASTRA_EH_NUMMER'])['N1_Demand'].transform('mean').round()\n",
    "    \n",
    "    # Replace closed observations\n",
    "    df.loc[is_closed, 'N1_Demand'] = group_means[is_closed]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_n2_uncensoring(df):\n",
    "    \"\"\"\n",
    "    N2: replace censored values with mean of uncensored values within each POS.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['N2_Demand'] = df['Verkauf'].copy()\n",
    "    \n",
    "    # # Calculate mean of uncensored observations for each group\n",
    "    # uncensored_means = (\n",
    "    #     df[df['Zensiert'] == 0]\n",
    "    #     .groupby(['EHASTRA_EH_NUMMER'])['Verkauf']\n",
    "    #     .mean()\n",
    "    #     .rename('uncensored_mean')\n",
    "    #     .reset_index()\n",
    "    #     .round()\n",
    "    # )\n",
    "    \n",
    "    # # Merge back to original DataFrame\n",
    "    # df = df.merge(uncensored_means, on=['EHASTRA_EH_NUMMER'], how='left')\n",
    "    \n",
    "    # # CREATE MASK AFTER MERGE - this is the key fix\n",
    "    # mask = (df['Zensiert'] == 1) & df['uncensored_mean'].notna()\n",
    "    # df.loc[mask, 'N2_Demand'] = df.loc[mask, 'uncensored_mean']\n",
    "    \n",
    "    # return df.drop('uncensored_mean', axis=1)\n",
    "    # mean only over uncensored values\n",
    "    uncensored_means = (\n",
    "        df.loc[df['Zensiert'] == 0]\n",
    "        .groupby('EHASTRA_EH_NUMMER')['Verkauf']\n",
    "        .transform('mean')\n",
    "    )\n",
    "\n",
    "    # align values directly back to df\n",
    "    df.loc[df['Zensiert'] == 1, 'N2_Demand'] = (\n",
    "        df.loc[df['Zensiert'] == 1, 'EHASTRA_EH_NUMMER']\n",
    "        .map(df.loc[df['Zensiert'] == 0].groupby('EHASTRA_EH_NUMMER')['Verkauf'].mean().round())\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_n3_uncensoring(df):\n",
    "    \"\"\"\n",
    "    N3: replace censored values with max(current value, average of non-censored values) within each POS.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['N3_Demand'] = df['Verkauf'].copy()\n",
    "    \n",
    "    # Compute open group means and merge back\n",
    "    open_means = (\n",
    "        df[df['Zensiert'] == 0]\n",
    "        .groupby('EHASTRA_EH_NUMMER')['Verkauf']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'Verkauf': 'open_mean'})\n",
    "        .round()\n",
    "    )\n",
    "    \n",
    "    df = df.merge(open_means, on='EHASTRA_EH_NUMMER', how='left')\n",
    "    \n",
    "    # CREATE MASK AFTER MERGE - this is the key fix\n",
    "    mask = (df['Zensiert'] == 1) & df['open_mean'].notna()\n",
    "    df.loc[mask, 'N3_Demand'] = np.maximum(\n",
    "        df.loc[mask, 'Verkauf'],\n",
    "        df.loc[mask, 'open_mean']\n",
    "    )\n",
    "    \n",
    "    return df.drop('open_mean', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1efcfc",
   "metadata": {},
   "source": [
    "## EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "773729fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "def apply_em_uncensoring(df, max_iter=30, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - max_iter: Maximum number of iterations\n",
    "    - tolerance: Convergence tolerance\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['EM_Demand'] = df['Verkauf'].copy()\n",
    "    \n",
    "    stockout_condition = (df['Zensiert'] == 1)\n",
    "    \n",
    "    for (pos,), group in df.groupby(['EHASTRA_EH_NUMMER']):\n",
    "        try:\n",
    "            group_stockout = stockout_condition.loc[group.index]\n",
    "            \n",
    "            if not group_stockout.any():\n",
    "                continue\n",
    "            \n",
    "            sales = group['EM_Demand'].values\n",
    "            is_stockout = group_stockout.values\n",
    "            \n",
    "            uncensored = sales[~is_stockout]\n",
    "            censored = sales[is_stockout]\n",
    "            \n",
    "            # Skip if no uncensored data - but keep original values\n",
    "            if len(uncensored) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Quick lambda initialization\n",
    "            lambda_est = np.mean(uncensored) if len(uncensored) > 0 else np.mean(sales) * 1.5\n",
    "            lambda_est = max(lambda_est, 0.1)\n",
    "            \n",
    "            # Fast EM loop\n",
    "            for iteration in range(max_iter):\n",
    "                lambda_old = lambda_est\n",
    "                \n",
    "                # Batch E-step\n",
    "                surv_prob = 1 - poisson.cdf(censored - 1, lambda_est)\n",
    "                exact_prob = poisson.pmf(censored, lambda_est)\n",
    "                surv_prob = np.maximum(surv_prob, 1e-12)\n",
    "                \n",
    "                expected = lambda_est + censored * exact_prob / surv_prob\n",
    "                expected = np.maximum(expected, censored.astype(float))\n",
    "                \n",
    "                # M-step\n",
    "                lambda_est = max(np.mean(np.concatenate([uncensored, expected])), 0.1)\n",
    "                \n",
    "                if abs(lambda_est - lambda_old) < tolerance:\n",
    "                    break\n",
    "            \n",
    "            # Final update\n",
    "            surv_prob = 1 - poisson.cdf(censored - 1, lambda_est)\n",
    "            exact_prob = poisson.pmf(censored, lambda_est)\n",
    "            surv_prob = np.maximum(surv_prob, 1e-12)\n",
    "            \n",
    "            final_expected = lambda_est + censored * exact_prob / surv_prob\n",
    "            final_expected = np.maximum(final_expected, censored.astype(float))\n",
    "            \n",
    "            # Update original dataframe\n",
    "            stockout_indices = group.index[is_stockout]\n",
    "            df.loc[stockout_indices, 'EM_Demand'] = final_expected.round()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"EM error for POS {pos}: {e}\")\n",
    "            # Fill with original Verkauf values for this POS when error occurs\n",
    "            group_stockout_indices = group.index[stockout_condition.loc[group.index]]\n",
    "            df.loc[group_stockout_indices, 'EM_Demand'] = df.loc[group_stockout_indices, 'Verkauf']\n",
    "            continue\n",
    "    \n",
    "    # Replace any NaN values with original Verkauf\n",
    "    nan_mask = df['EM_Demand'].isna()\n",
    "    df.loc[nan_mask, 'EM_Demand'] = df.loc[nan_mask, 'Verkauf']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228eb84a",
   "metadata": {},
   "source": [
    "## PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0a25eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "def apply_pd_uncensoring(df, tau=0.5, max_iter=20, tolerance=1e-4):\n",
    "    \"\"\"PD with skip for invalid POS groups and fallback to original Verkauf\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"is_closed\"] = (df[\"Zensiert\"] == 1)\n",
    "    df['PD_Demand'] = df['Verkauf'].copy().astype(float)\n",
    "\n",
    "    def compute_pd_projection1(obs_val, lambda_est, tau):\n",
    "        \"\"\"Your PD projection with NaN protection\"\"\"\n",
    "        try:\n",
    "            # Check for NaN inputs\n",
    "            if pd.isna(obs_val) or pd.isna(lambda_est) or lambda_est <= 0:\n",
    "                return float(obs_val) if not pd.isna(obs_val) else 0.0\n",
    "            \n",
    "            obs_val = int(round(obs_val))\n",
    "            \n",
    "            def objective(k_proj):\n",
    "                k_proj = int(round(k_proj))\n",
    "                \n",
    "                if k_proj < obs_val:\n",
    "                    return float('inf')\n",
    "                \n",
    "                # Area A: P(obs_val <= X <= k_proj)\n",
    "                area_A = poisson.cdf(k_proj, lambda_est) - poisson.cdf(obs_val - 1, lambda_est)\n",
    "                \n",
    "                # Area B: P(X > k_proj)\n",
    "                area_B = 1 - poisson.cdf(k_proj, lambda_est)\n",
    "                \n",
    "                if area_B > 1e-10:\n",
    "                    ratio = area_A / area_B\n",
    "                    target_ratio = (1 - tau) / tau\n",
    "                    return abs(ratio - target_ratio)\n",
    "                else:\n",
    "                    return abs(area_A - (1 - tau))\n",
    "            \n",
    "            # Search for optimal projection\n",
    "            upper_bound = int(obs_val + max(10, int(3 * np.sqrt(lambda_est))))\n",
    "            \n",
    "            best_k = obs_val\n",
    "            best_objective = float('inf')\n",
    "            \n",
    "            for k in range(int(obs_val), upper_bound + 1):\n",
    "                obj_val = objective(k)\n",
    "                if obj_val < best_objective:\n",
    "                    best_objective = obj_val\n",
    "                    best_k = k\n",
    "            \n",
    "            return float(best_k)\n",
    "            \n",
    "        except Exception:\n",
    "            # Fallback to original value\n",
    "            return float(obs_val) if not pd.isna(obs_val) else 0.0\n",
    "    \n",
    "    # Process groups\n",
    "    grouped = df.groupby('EHASTRA_EH_NUMMER')\n",
    "    \n",
    "    for pos, group in grouped:\n",
    "        try:\n",
    "            open_mask = ~group['is_closed']\n",
    "            closed_mask = group['is_closed']\n",
    "            \n",
    "            open_sales = group.loc[open_mask, 'PD_Demand'].values\n",
    "            closed_sales = group.loc[closed_mask, 'PD_Demand'].values\n",
    "            \n",
    "            # SKIP POS WITHOUT VALID UNCENSORED DATA\n",
    "            if len(open_sales) == 0 or np.all(pd.isna(open_sales)):\n",
    "                # print(f\"Skipping POS {pos}: no valid uncensored data\")\n",
    "                continue\n",
    "            \n",
    "            # Initialize lambda parameter\n",
    "            lambda_est = np.mean(open_sales[~pd.isna(open_sales)])\n",
    "            \n",
    "            # SKIP IF LAMBDA IS INVALID\n",
    "            if pd.isna(lambda_est) or lambda_est <= 0:\n",
    "                print(f\"Skipping POS {pos}: invalid lambda {lambda_est}\")\n",
    "                continue\n",
    "            \n",
    "            lambda_est = max(lambda_est, 0.1)\n",
    "            closed_indices = group[closed_mask].index.values\n",
    "            \n",
    "            # Iterative process\n",
    "            for iteration in range(max_iter):\n",
    "                lambda_old = lambda_est\n",
    "                \n",
    "                # Project closed observations\n",
    "                projected_values = np.array([\n",
    "                    compute_pd_projection1(obs, lambda_est, tau)\n",
    "                    for obs in closed_sales\n",
    "                ])\n",
    "                \n",
    "                # Re-estimate lambda\n",
    "                all_values = np.concatenate([open_sales, projected_values])\n",
    "                lambda_est = np.mean(all_values)\n",
    "                lambda_est = max(lambda_est, 0.1)\n",
    "                \n",
    "                # Check convergence\n",
    "                if abs(lambda_est - lambda_old) < tolerance:\n",
    "                    break\n",
    "            \n",
    "            # Final projection\n",
    "            final_projections = np.array([\n",
    "                compute_pd_projection1(obs, lambda_est, tau)\n",
    "                for obs in closed_sales\n",
    "            ])\n",
    "            \n",
    "            # Update dataframe\n",
    "            df.loc[closed_indices, 'PD_Demand'] = final_projections.round()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"PD error for POS {pos}: {e}\")\n",
    "            # FALLBACK: Keep original values for this POS\n",
    "            continue\n",
    "    \n",
    "    # FINAL FALLBACK: Any remaining NaN values get original Verkauf\n",
    "    nan_mask = df['PD_Demand'].isna()\n",
    "    df.loc[nan_mask, 'PD_Demand'] = df.loc[nan_mask, 'Verkauf']\n",
    "    \n",
    "    return df.drop('is_closed', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2141346",
   "metadata": {},
   "source": [
    "## Conrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b85a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "def berechnung(links, rechts, n, N, r, x_summe, value_tol=0.00001, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Till's Code\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        mu = (links + rechts) / 2\n",
    "        wert_0 = (x_summe - mu * n) * (1 - poisson.cdf(N-1, mu)) + mu * (n - r) * (1 - poisson.cdf(N-2, mu))\n",
    "        \n",
    "        # if iteration < 3:\n",
    "            #print(f\"Iter {iteration}: mu={mu:.4f}, wert_0={wert_0:.8f}\")\n",
    "        \n",
    "        if abs(wert_0) < value_tol:\n",
    "            #print(f\"Converged after {iteration} iterations: mu={mu:.4f}, wert_0={wert_0:.8f}\")\n",
    "            return mu\n",
    "        elif wert_0 < 0:\n",
    "            rechts = mu\n",
    "        elif wert_0 > 0:\n",
    "            links = mu\n",
    "            \n",
    "        iteration += 1\n",
    "    \n",
    "    #print(f\"✗ Max iterations reached: mu={mu:.4f}\")\n",
    "    return mu\n",
    "\n",
    "def test_conrad_example():\n",
    "    links = 1\n",
    "    rechts = 100\n",
    "    n = 13\n",
    "    N = 10\n",
    "    r = 7\n",
    "    x_summe = 58\n",
    "    \n",
    "    print(f\"links={links}, rechts={rechts}\")\n",
    "    print(f\"n={n}, N={N}, r={r}, x_summe={x_summe}\")\n",
    "    \n",
    "    result = berechnung(links, rechts, n, N, r, x_summe)\n",
    "    print(f\"Result: μ = {result:.4f}\")\n",
    "    print(f\"Expected: μ ≈ 10.18\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_order_specific_mu_dict(df):\n",
    "\n",
    "    order_specific_mu_dict = {}\n",
    "    \n",
    "    for (year, week), week_data in df.groupby(['Heftjahr', 'Heftnummer']):\n",
    "        for bezug_val, group in week_data.groupby('Bezug'):\n",
    "            n = len(group)\n",
    "            N = bezug_val\n",
    "            \n",
    "            # Count non-stockouts\n",
    "            stockouts_mask = (group['Zensiert'] == 1)\n",
    "            r = n - stockouts_mask.sum()  # r = number of NON-stockouts\n",
    "            \n",
    "            # x_summe = sum of UNCENSORED observations only\n",
    "            uncensored_sales = group[~stockouts_mask]['Verkauf']\n",
    "            x_summe = uncensored_sales.sum()\n",
    "            \n",
    "            # Skip problematic cases\n",
    "            if n < 3:\n",
    "                continue\n",
    "            if r == n:  # No stockouts = no censoring information\n",
    "                continue\n",
    "            if r == 0:  # All stockouts = no uncensored observations\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                links = 1\n",
    "                rechts = 100\n",
    "                mu_est = berechnung(links, rechts, n, N, r, x_summe)\n",
    "                if mu_est:\n",
    "                    key = (year, week, bezug_val)\n",
    "                    order_specific_mu_dict[key] = mu_est\n",
    "            except Exception as e:\n",
    "                print(f\"Error in week {week}, Bezug {N}: {e}\")\n",
    "                continue\n",
    "\n",
    "    #print(f\"Successfully estimated μ for {len(order_specific_mu_dict)} groups\")\n",
    "    return order_specific_mu_dict\n",
    "\n",
    "def expected_poisson_tail(mu, N, max_k=200):\n",
    "    \"\"\"\n",
    "    Compute E[X | X >= N] for X ~ Poisson(mu)\n",
    "    \"\"\"\n",
    "    k_vals = np.arange(N, max_k)\n",
    "    pmf = poisson.pmf(k_vals, mu)\n",
    "    tail_prob = 1 - poisson.cdf(N - 1, mu)\n",
    "    if tail_prob < 1e-8:\n",
    "        return N  # fallback: don't uncensor\n",
    "    return np.sum(k_vals * pmf) / tail_prob\n",
    "\n",
    "# Internal function\n",
    "def apply_conrad_uncensoring_1(df, order_specific_mu_dict):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with Verkauf, Bezug, is_stockout, Heftjahr, Heftnummer,\n",
    "    replace Verkauf with E[X | X >= Bezug] when censored.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['Conrad_Demand'] = df['Verkauf'].copy()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Skip if no stockout occurred\n",
    "        if row['Zensiert'] == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get the order-quantity-specific demand parameter\n",
    "        key = (row['Heftjahr'], row['Heftnummer'], row['Bezug'])\n",
    "        mu = order_specific_mu_dict.get(key, None)\n",
    "\n",
    "        if mu is None:\n",
    "            # no estimate available for this specific (week, order_quantity) — keep original value\n",
    "            continue\n",
    "\n",
    "        # POS sold out — uncensor using the specific distribution for this order quantity\n",
    "        est_demand = expected_poisson_tail(mu, row['Bezug'])\n",
    "        df.at[idx, 'Conrad_Demand'] = np.round(est_demand)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Wrapper function for script\n",
    "def apply_conrad_uncensoring(df):\n",
    "    \"\"\"\n",
    "    WRAPPER FUNCTION: This is what gets called by the main processing loop\n",
    "    \"\"\"\n",
    "    # Step 1: Create mu dictionary from the dataset\n",
    "    order_specific_mu_dict = create_order_specific_mu_dict(df)\n",
    "    \n",
    "    # Step 2: Apply uncensoring using the mu dictionary\n",
    "    return apply_conrad_uncensoring_1(df, order_specific_mu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e33a2",
   "metadata": {},
   "source": [
    "## Nahmias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2cfdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def nahmias_estimation(sales, S):\n",
    "    \"\"\"\n",
    "    Nahmias method for censored normal data\n",
    "    \"\"\"\n",
    "    sales = np.array(sales)\n",
    "    n = len(sales)\n",
    "    \n",
    "    observed = sales[sales < S]\n",
    "    r = len(observed)\n",
    "    p = r / n\n",
    "    \n",
    "    if r < 2 or r >= n-1 or p <= 0 or p >= 1:\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        x_bar = np.mean(observed)\n",
    "        s2 = np.var(observed, ddof=1)\n",
    "        z = norm.ppf(p)\n",
    "        \n",
    "        sigma_hat2 = s2 / (1 - (z * norm.pdf(z) / p) - (norm.pdf(z)**2 / p**2))\n",
    "        sigma_hat = np.sqrt(sigma_hat2)\n",
    "        mu_hat = x_bar + sigma_hat * norm.pdf(z) / p\n",
    "        \n",
    "        if not np.isfinite(mu_hat) or not np.isfinite(sigma_hat) or sigma_hat <= 0:\n",
    "            return None, None\n",
    "            \n",
    "        return mu_hat, sigma_hat\n",
    "        \n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def create_order_specific_nahmias_dict(df):\n",
    "    \"\"\"\n",
    "    Create μ and σ estimates for each (week, order_quantity) combination\n",
    "    \"\"\"\n",
    "    order_specific_mu_dict = {}\n",
    "    order_specific_sigma_dict = {}\n",
    "\n",
    "    for (year, week), week_data in df.groupby(['Heftjahr', 'Heftnummer']):\n",
    "        for bezug_val, group in week_data.groupby('Bezug'):\n",
    "            n = len(group)\n",
    "            S = bezug_val\n",
    "            \n",
    "            if n < 5:\n",
    "                continue\n",
    "            \n",
    "            sales = group['Verkauf'].values\n",
    "            \n",
    "            try:\n",
    "                mu_est, sigma_est = nahmias_estimation(sales, S)\n",
    "                if mu_est is not None and sigma_est is not None:\n",
    "                    key = (year, week, bezug_val)\n",
    "                    order_specific_mu_dict[key] = mu_est\n",
    "                    order_specific_sigma_dict[key] = sigma_est\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    # print(f\"Successfully estimated μ,σ for {len(order_specific_mu_dict)} groups\")\n",
    "    return order_specific_mu_dict, order_specific_sigma_dict\n",
    "\n",
    "def expected_normal_tail(mu, sigma, S):\n",
    "    \"\"\"\n",
    "    Compute E[X | X >= S] for X ~ Normal(mu, sigma)\n",
    "    \"\"\"\n",
    "    if sigma <= 0:\n",
    "        return S\n",
    "    \n",
    "    z = (S - mu) / sigma\n",
    "    \n",
    "    if z > 6:\n",
    "        return S\n",
    "    \n",
    "    tail_prob = 1 - norm.cdf(z)\n",
    "    \n",
    "    if tail_prob < 1e-10:\n",
    "        return S\n",
    "    \n",
    "    expected_value = mu + sigma * norm.pdf(z) / tail_prob\n",
    "    \n",
    "    return expected_value\n",
    "\n",
    "def apply_nahmias_uncensoring_1(df, order_specific_mu_dict, order_specific_sigma_dict):\n",
    "    \"\"\"\n",
    "    Uncensor dataset using Nahmias estimates\n",
    "    \"\"\"\n",
    "    #df = df.copy()\n",
    "    df['Nahmias_Demand'] = df['Verkauf'].copy()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['Zensiert'] == 0:\n",
    "            continue\n",
    "            \n",
    "        key = (row['Heftjahr'], row['Heftnummer'], row['Bezug'])\n",
    "        mu = order_specific_mu_dict.get(key, None)\n",
    "        sigma = order_specific_sigma_dict.get(key, None)\n",
    "\n",
    "        if mu is None or sigma is None:\n",
    "            continue\n",
    "\n",
    "        est_demand = expected_normal_tail(mu, sigma, row['Bezug'])\n",
    "        df.at[idx, 'Nahmias_Demand'] = np.round(est_demand)\n",
    "\n",
    "    return df\n",
    "\n",
    "def test_nahmias():\n",
    "    \"\"\"Test implementation\"\"\"\n",
    "    mu_true = 100\n",
    "    sigma_true = 30\n",
    "    S = 110\n",
    "    n = 100\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    demand = np.random.normal(mu_true, sigma_true, n)\n",
    "    sales = np.minimum(demand, S)\n",
    "    \n",
    "    print(\"Testing Nahmias implementation:\")\n",
    "    print(f\"True μ: {mu_true}, True σ: {sigma_true}\")\n",
    "    print(f\"S (censoring limit): {S}\")\n",
    "    print(f\"Sample size: {n}\")\n",
    "    \n",
    "    mu_hat, sigma_hat = nahmias_estimation(sales, S)\n",
    "    \n",
    "    naive_mean = np.mean(sales)\n",
    "    naive_std = np.std(sales, ddof=1)\n",
    "    \n",
    "    print(f\"True mean: {mu_true}\")\n",
    "    print(f\"Naive mean (sales): {naive_mean:.2f}\")\n",
    "    print(f\"Corrected estimator (Nahmias): {mu_hat:.2f}\")\n",
    "    print(f\"True Std.Dev.: {sigma_true}\")\n",
    "    print(f\"Corrected Std.Dev.: {sigma_hat:.2f}\")\n",
    "    print(f\"Naive Std.Dev. (sales): {naive_std:.2f}\")\n",
    "    \n",
    "    return mu_hat, sigma_hat\n",
    "\n",
    "def apply_nahmias_uncensoring(df):\n",
    "    \"\"\"\n",
    "    WRAPPER FUNCTION: This is what gets called by the main processing loop\n",
    "    \"\"\"\n",
    "    # Step 1: Create mu dictionary from the dataset\n",
    "    order_specific_mu_dict, compute_mu_sigma_nahmias = create_order_specific_nahmias_dict(df)\n",
    "    \n",
    "    # Step 2: Apply uncensoring using the mu dictionary\n",
    "    return apply_nahmias_uncensoring_1(df, order_specific_mu_dict, compute_mu_sigma_nahmias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a4066",
   "metadata": {},
   "source": [
    "## Agrawal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6b3e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import nbinom\n",
    "from scipy.optimize import minimize_scalar\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def find_p(N, bezug, F_observed):\n",
    "    \"\"\"\"\n",
    "    Find p for a given N such that P(X < S) from the Negative Binomial\n",
    "    matches the empirically observed uncensored probability.\n",
    "    \"\"\"\n",
    "    func = lambda p: abs(nbinom.cdf(bezug - 1, N, p) - F_observed)\n",
    "    res = minimize_scalar(func, bounds=(0.01, 0.99), method='bounded')\n",
    "    return res.x\n",
    "\n",
    "\n",
    "def uncensor(N_hat, p_hat, s, max):\n",
    "    '''\n",
    "    Compute E[X|X>=s] for X~NB(N_hat,p_hat).\n",
    "    max: upper demand limit used in calculation\n",
    "    '''\n",
    "    X_vals = np.arange(s, max)\n",
    "    pmf = nbinom.pmf(X_vals, N_hat, p_hat)\n",
    "    tail_prob = 1- nbinom.cdf(s-1, N_hat, p_hat)\n",
    "    if tail_prob < 1e-8:\n",
    "        return s  # fallback: don't uncensor\n",
    "    exp = np.sum(X_vals * pmf) / tail_prob\n",
    "    return int(np.round(exp))\n",
    "\n",
    "\n",
    "def compute_NB_result(group_key_data):\n",
    "    '''\n",
    "    Given one hierarchical group, calculate the Agrawal demand\n",
    "    '''\n",
    "    def objective(N, bezug, x_bar, F_observed):\n",
    "        p = find_p(N, bezug, F_observed)\n",
    "        prob_uncensored = nbinom.cdf(bezug - 1, N, p)\n",
    "        if prob_uncensored == 0.0: return 10**8 #if probability of demand being uncensored is too low, return a high objective value\n",
    "        expect = nbinom.expect(lambda x: x, args=(N, p), lb=0, ub=bezug-1)\n",
    "        mean_uncensored = expect / prob_uncensored\n",
    "        return abs(mean_uncensored - x_bar)\n",
    "    \n",
    "\n",
    "    (bezug, period), group = group_key_data\n",
    "    group = group.dropna(subset=['Bezug', 'Verkauf'])\n",
    "\n",
    "    group_uncensored = group[group['Zensiert']==0]\n",
    "    x_bar = group_uncensored['Verkauf'].mean()\n",
    "    F_observed = len(group_uncensored)/len(group)\n",
    "\n",
    "    result = minimize_scalar(lambda N: objective(N,bezug,x_bar,F_observed), bounds=(0.1, 500), method='bounded')\n",
    "    N_hat = result.x\n",
    "    p_hat = find_p(N_hat, bezug, F_observed)\n",
    "\n",
    "    ub = max(100, bezug*2)\n",
    "    demand = uncensor(N_hat, p_hat, bezug, max = ub)\n",
    "    #print(f'Period: {period}, Bezug: {bezug}, N: {N_hat}, p: {p_hat}')\n",
    "    return {'Period': period, 'Bezug': bezug, 'Agrawal_Demand': demand}\n",
    "\n",
    "\n",
    "def apply_agrawal_uncensoring(df, max_cpus=1):\n",
    "    '''\n",
    "    Group magazine data by bezug and period run Agrawal algorithm in parallel.\n",
    "    Returns original dataframe with new columns Agrawal_Demand\n",
    "    '''\n",
    "    #Step 1: calculate Agrawal demand\n",
    "    df.sort_values(['Bezug', 'Period'], inplace=True)\n",
    "    grouped = list(df.groupby(['Bezug', 'Period']))\n",
    "    results = []\n",
    "\n",
    "    # with ThreadPoolExecutor(max_workers=max_cpus) as executor:\n",
    "    #     futures = [executor.submit(compute_NB_result, group) for group in grouped]\n",
    "    #     for future in as_completed(futures):\n",
    "    #         results.append(future.result())\n",
    "    for group in grouped:\n",
    "        results.append(compute_NB_result(group))\n",
    "    df_results= pd.DataFrame(results)\n",
    "\n",
    "    #Step 2: calculate hierarchical group uncensored fraction)\n",
    "    group_data = []\n",
    "    for (bezug,period),group in df.groupby(['Bezug', 'Period']):\n",
    "        group_data.append({'Bezug':bezug, 'Period':period, 'Uncensored_fraction': len(group[group['Zensiert']==0])/len(group)})\n",
    "    df_group = pd.DataFrame(group_data)\n",
    "    df_results = pd.merge(df_results, df_group, how='outer', on=['Bezug','Period'])\n",
    "\n",
    "    #Step 3: add results to original data\n",
    "    df_join = pd.merge(df, df_results, how=\"outer\", on=['Bezug','Period'])\n",
    "\n",
    "    #only use Agrawal estimation for censored data (use real sales value for not censored data)\n",
    "    df_join['Agrawal_Demand'] = np.where(df_join['Zensiert'], df_join['Agrawal_Demand'], df_join['Verkauf'])\n",
    "\n",
    "    #only use Agrawal estimation for valid results (uncensored fraction is not 0 or 1)\n",
    "    df_join['Agrawal_Demand'] = np.where((df_join['Uncensored_fraction']==0.0)|(df_join['Uncensored_fraction']==1.0), df_join['Verkauf'], df_join['Agrawal_Demand'])\n",
    "    \n",
    "    df_join.drop(['Uncensored_fraction'],axis='columns', inplace=True)\n",
    "    return df_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fa76bc",
   "metadata": {},
   "source": [
    "## Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7974fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import nbinom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86b176",
   "metadata": {},
   "source": [
    "### Precomputations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b1eec",
   "metadata": {},
   "source": [
    "To improve runtime of Bayesian method, precompute PMF and CMF of negative binomial distribution. Also precompute conditional sales probabilities.<br>\n",
    "note: $p_{idx}$ corresponds to the index in the grid $0.900, 0.901, ..., 0.990$\n",
    "<br><br>\n",
    "pmf_table[$n,r,p_{idx}$] = nbinom.pmf($n,r,p$) where $p = 0.900+p_{idx}*0.001$<br>\n",
    "cmf_table[$n,r,p_idx$] = nbinom.cmf($n,r,p$) where $p = 0.900+p_{idx}*0.001$<br>\n",
    "cond_pmf_sales[$X, s, r, p_{idx}$] = P($X$ sales with $s$ inventory and demand ~ NB($r,p$)) where $p = 0.900+p_{idx}*0.001$<br>\n",
    "<br>\n",
    "We choose to precompute above values up to $n=200, r=300, X=200, s=200$ as these upper bounds will cover most data points. For remaining cases not included in the tables, we directly compute them.\n",
    "<br>\n",
    "If you do not want to use precomputations, simply set the max_n and max_r variables to $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18627e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 301, 91)\n",
      "(201, 301, 91)\n",
      "(201, 201, 301, 91)\n"
     ]
    }
   ],
   "source": [
    "#upper bounds for initial NB precomputing\n",
    "max_n = 200\n",
    "max_r = 300\n",
    "\n",
    "#non-parametric p distribution values for NB fitting\n",
    "p_grid = np.linspace(0.900, 0.990, 91)\n",
    "\n",
    "def precompute_NB ():\n",
    "    '''\n",
    "    Calculates tables containing NB pmf and cmf values up to given bounds.\n",
    "    Output: two tables with shape (max_n, max_r, len(p_grid))\n",
    "    '''\n",
    "    pmf_table = np.zeros((max_n+1, max_r +1, len(p_grid)))\n",
    "    cmf_table = np.zeros((max_n+1, max_r +1, len(p_grid)))\n",
    "    for p_idx, p in enumerate(p_grid):\n",
    "        for r in range(max_r+1):\n",
    "            pmf_table[:, r, p_idx] = nbinom.pmf(np.arange(0,max_n+1), r, p)\n",
    "            cmf_table[:, r, p_idx] = nbinom.cdf(np.arange(0,max_n+1), r, p)\n",
    "    np.save(\"pmf_table.npy\", pmf_table)\n",
    "    np.save(\"cmf_table.npy\", cmf_table)\n",
    "\n",
    "def precompute_cond_pmf_sale():\n",
    "    '''\n",
    "    Calculates table containing conditional pmf sales (given demand~NB(r,p) and inventory is s)\n",
    "    For uncensored sales (X < s): table returns nbinom.pmf(X, r, p)\n",
    "    For censored sales (X >= s): table returns 1 - nbinom.cmf(s-1, r, p)\n",
    "\n",
    "    Output: one table with shape (max_n, max_n, max_r, len(p_grid))\n",
    "    Specifically, value at index (X, s, r, p) represents probability of getting X sales given demand~NB(r,p) and inventory is s\n",
    "    '''\n",
    "    cond_pmf_sale = np.zeros((max_n+1, max_n+1, max_r+1, len(p_grid)))\n",
    "\n",
    "    for p_idx, p in enumerate(p_grid):\n",
    "        for X in range(max_n+1):\n",
    "            for s in range(max_n+1):\n",
    "                for r in range(max_r+1):\n",
    "                    cond_pmf_sale[X, s, r, p_idx] = pmf_table[X, r, p_idx] if X < s else 1 - cmf_table[s-1, r, p_idx]\n",
    "    np.save(\"cond_pmf_sale_table.npy\", cond_pmf_sale)\n",
    "\n",
    "#precompute_NB()\n",
    "pmf_table = np.load('pmf_table.npy')\n",
    "cmf_table = np.load('cmf_table.npy')\n",
    "#precompute_cond_pmf_sale()\n",
    "cond_pmf_sales_table = np.load('cond_pmf_sale_table.npy')\n",
    "\n",
    "print(pmf_table.shape) # (201,301,91)\n",
    "print(cmf_table.shape) # (201,301,91)\n",
    "print(cond_pmf_sales_table.shape) # (201,201,301,91)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db276c2f",
   "metadata": {},
   "source": [
    "### Bayesian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e456dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_quantity(r, c_ratio, pmf_p, lo=0, hi=100):\n",
    "    '''\n",
    "    Returns minimum order quantity such that predicted demand is above the critical ratio\n",
    "    '''\n",
    "    # Use binary search to determine lowest value such that quantile > critical ratio\n",
    "    # quantile at mid = ∑_(demand = 0 to mid)∑_(p_grid)  pmf(demand, r, p)*π(p)\n",
    "    while lo < hi:\n",
    "        mid = (lo + hi) // 2\n",
    "        \n",
    "        if mid > max_n or r > max_r:\n",
    "            quantile =  np.sum([np.dot(nbinom.pmf(y, r, p_grid), pmf_p) for y in range(int(mid) + 1)])\n",
    "        else:\n",
    "            quantile = np.sum([np.dot(pmf_table[y, r, :], pmf_p) for y in range(int(mid) + 1)])\n",
    "\n",
    "        if quantile < c_ratio:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            hi = mid\n",
    "    return lo\n",
    "\n",
    "def uncensor(r, pmf_p, s, max):\n",
    "    '''\n",
    "    Calculate E[X|X>=s, X~NB(r, p)] with p distribution = pmf_p\n",
    "    max: upper demand limit used in calculation\n",
    "    '''\n",
    "    # tail_prob = probability of demand >=s = ∑_(p_grid) sf(s, r, p)*π(p)\n",
    "    # pmf = pmf of demand s, s+1, ..., max (entry for demand x = ∑_(p_grid) pmf(x, r, p)*π(p))\n",
    "    if r > max_r or max > max_n:\n",
    "        pmf = np.dot(nbinom.pmf(np.arange(s,max+1)[:, None], r, p_grid), pmf_p) #length = max+1-s\n",
    "        tail_prob = np.dot(1 - nbinom.cdf(s-1, r, p_grid)[None, :], pmf_p).item()\n",
    "    else:\n",
    "        pmf = np.dot(pmf_table[s:max+1, r,:], pmf_p)\n",
    "        tail_prob = np.dot(1 - cmf_table[s-1, r,:], pmf_p)\n",
    "\n",
    "    if tail_prob < 1e-8:\n",
    "        return s  # fallback: don't uncensor (demand>=s is unlikely)\n",
    "    \n",
    "    exp = np.dot(np.arange(s,max+1), pmf)/tail_prob\n",
    "    return int(np.round(exp))\n",
    "\n",
    "def compute_bayesian_result(group_key_data, c_ratio):\n",
    "    '''\n",
    "    given one pos group and critical ratio, calculate the bayesian demand and order quantity\n",
    "    if there is no 2022-2023 data or if mean 2022-2023 sales is too low (<0.052), this pos is skipped\n",
    "    '''\n",
    "    (pos,), group = group_key_data\n",
    "    group = group.dropna(subset=['Bezug', 'Verkauf'])\n",
    "    results = []\n",
    "\n",
    "    #determine appropriate r value using 2022-2023 mean sales\n",
    "    train_data = group[group['Heftjahr']<2024]\n",
    "    if len(train_data) == 0:\n",
    "        #print(f'skipped, {pos} has no 2022-2023 data points')\n",
    "        return\n",
    "    else:\n",
    "        mean_2223 = train_data['Verkauf'].mean()\n",
    "        r = int(mean_2223*0.95/0.05) #NB(r, 0.95) has mean ≈ mean(2022-2023 sales)\n",
    "        if r == 0.0:\n",
    "            print(f'Bayesian skipped for {pos}, r = 0')\n",
    "            return #don't use Bayesian for this POS\n",
    "\n",
    "    # initialize distribution of p, π, to Uniform\n",
    "    p_grid = np.linspace(0.900, 0.990, 91)\n",
    "    pmf_p = np.full(len(p_grid), 1.0 / len(p_grid))\n",
    "\n",
    "    #update π with each new verkauf-bezug data point\n",
    "    for row in group.itertuples():\n",
    "        #cond_sale_probs = probability of sale given demand~NB(r,p) and inventory level\n",
    "        if row.Bezug > max_n or r > max_r:\n",
    "            cond_sale_probs = [nbinom.pmf(row.Verkauf, r, p) if row.Verkauf < row.Bezug else 1-nbinom.cdf(row.Bezug - 1, r, p) for p in p_grid] #shape = len(p_grid)\n",
    "        else:\n",
    "            cond_sale_probs = cond_pmf_sales_table[int(row.Verkauf), int(row.Bezug), r, :]  #shape = len(p_grid)\n",
    "            \n",
    "        sale_prob = np.dot(cond_sale_probs, pmf_p) #summed over all p values\n",
    "        if pmf_p.size == 0:\n",
    "            print(pmf_p)\n",
    "            print(row)\n",
    "        #do not update π if probability of sale is too low\n",
    "        if sale_prob != 0.0:\n",
    "            pmf_p = pmf_p * cond_sale_probs / sale_prob\n",
    "        \n",
    "        ub = max(50,2*int(train_data['Verkauf'].max()))\n",
    "        Q = quantile_quantity(r, c_ratio, pmf_p, lo=0, hi=ub)\n",
    "        if Q == ub: print(f'warning: {pos}, {row.Period}, Q is limited by upper bound of {Q}') #check if upper bound is limiting Q output\n",
    "\n",
    "        if row.Zensiert == 1: demand = uncensor(r, pmf_p, int(row.Bezug), max= ub)\n",
    "        else: demand = row.Verkauf\n",
    "\n",
    "        results.append({'EHASTRA_EH_NUMMER': pos, 'Period':row.Period, 'Bayesian_Demand': demand, 'Bayesian_Q': Q})\n",
    "\n",
    "    #print(f'{pos}, r = {r}, final pmf_p: {pmf_p}')\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def apply_bayesian_uncensoring_optimization(df, c_ratio=0.9, max_cpus = 1):\n",
    "    '''\n",
    "    Group magazine data by pos and run bayesian algorithm in parallel.\n",
    "    Returns original dataframe with new columns Bayesian_Demand, and Bayesian_Q\n",
    "    '''\n",
    "    #Step 1: calculate Bayesian Demand and Q values\n",
    "    df.sort_values(['EHASTRA_EH_NUMMER', 'Period'], inplace=True)\n",
    "    grouped = df.groupby(['EHASTRA_EH_NUMMER'])\n",
    "    grouped = list(grouped)\n",
    "    results = []\n",
    "\n",
    "    for group in grouped:\n",
    "        bayesian_result = compute_bayesian_result(group, c_ratio=0.9)\n",
    "        if bayesian_result is not None: results.append(bayesian_result)\n",
    "    df_Bayesian = pd.concat(results)\n",
    "\n",
    "    #Step 2: add results to original dataframe\n",
    "    df_join = pd.merge(df, df_Bayesian, how=\"outer\", on=['EHASTRA_EH_NUMMER', 'Period'])\n",
    "    df_join.set_index(['EHASTRA_EH_NUMMER', 'Period'])\n",
    "\n",
    "    # Step 3: For POS where Bayesian does not work, fill in missing demand predictions with verkauf. Missing Q values are not changed\n",
    "    df_join.loc[df_join['Verkauf'].notna() & df_join['Bayesian_Demand'].isna(), 'Bayesian_Demand'] = df_join['Verkauf']\n",
    "    return df_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670aa0a3",
   "metadata": {},
   "source": [
    "# Run Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1324cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_uncensoring(methods, dataframes):\n",
    "    '''\n",
    "    methods: list of uncensoring methods to use\n",
    "    dataframes: list of original data (magazines) to use\n",
    "    Applies each method to all dataframes and saves a csv file for each method (original columns + method_Demand)\n",
    "    '''\n",
    "    method_functions = {\n",
    "        'N1': apply_n1_uncensoring,\n",
    "        'N2': apply_n2_uncensoring,\n",
    "        'N3': apply_n3_uncensoring,\n",
    "        'EM': apply_em_uncensoring,\n",
    "        'PD': apply_pd_uncensoring,\n",
    "        'Nahmias': apply_nahmias_uncensoring,\n",
    "        'Conrad': apply_conrad_uncensoring,\n",
    "        'Baseline': apply_baseline_uncensoring,\n",
    "        'Bayesian': apply_bayesian_uncensoring_optimization,\n",
    "        'Agrawal': apply_agrawal_uncensoring\n",
    "    }\n",
    "    df_all_uncensoring = pd.DataFrame()\n",
    "    for dataframe in dataframes:\n",
    "        df_magazine = None\n",
    "        for method in methods:\n",
    "            print(f'Running uncensoring with method {method} on magazine {dataframe['Magazine'].unique()[0]}')\n",
    "            df = method_functions[method](dataframe)\n",
    "            if df_magazine is None:\n",
    "                df_magazine = df\n",
    "            else:\n",
    "                new_cols = [c for c in df.columns if c not in df_magazine.columns or c in ['Period','EHASTRA_EH_NUMMER']]\n",
    "                df_magazine = pd.merge(df_magazine, df[new_cols], how = 'outer', on = ['Period', 'EHASTRA_EH_NUMMER'])\n",
    "        df_all_uncensoring = pd.concat([df_all_uncensoring, df_magazine], ignore_index=True)\n",
    "    \n",
    "    return df_all_uncensoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20e51c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ['EHA0017186' 'EHC0004006' 'EHC0005070' 'EHH0010681' 'EHB0023505'\n",
      " 'EHB0019844' 'EHE0003391' 'EHA0021360']\n",
      "1056\n",
      "B ['EHA0020265' 'EHD0015991' 'EHB0019622' 'EHB0023505' 'EHG0003391']\n",
      "660\n",
      "C ['EHB0023505']\n",
      "132\n",
      "D ['EHA0021360' 'EHB0019844' 'EHH0010681']\n",
      "177\n",
      "E ['EHB0019622' 'EHC0005070' 'EHE0003391']\n",
      "99\n",
      "F ['EHG0006538']\n",
      "62\n",
      "G ['EHB0023505' 'EHG0006538']\n",
      "132\n",
      "H ['EHB0019622' 'EHB0023505' 'EHG0006538' 'EHH0011851' 'EHI0022887']\n",
      "140\n",
      "I ['EHA0020265' 'EHH0010681' 'EHH0011851']\n",
      "54\n",
      "Running uncensoring with method N1 on magazine A\n",
      "Running uncensoring with method N2 on magazine A\n",
      "Running uncensoring with method N3 on magazine A\n",
      "Running uncensoring with method EM on magazine A\n",
      "Running uncensoring with method PD on magazine A\n",
      "Running uncensoring with method Nahmias on magazine A\n",
      "Running uncensoring with method Conrad on magazine A\n",
      "Running uncensoring with method Bayesian on magazine A\n",
      "Running uncensoring with method Baseline on magazine A\n",
      "Running uncensoring with method N1 on magazine B\n",
      "Running uncensoring with method N2 on magazine B\n",
      "Running uncensoring with method N3 on magazine B\n",
      "Running uncensoring with method EM on magazine B\n",
      "Running uncensoring with method PD on magazine B\n",
      "Skipping POS EHB0019622: invalid lambda 0.0\n",
      "Running uncensoring with method Nahmias on magazine B\n",
      "Running uncensoring with method Conrad on magazine B\n",
      "Running uncensoring with method Bayesian on magazine B\n",
      "Bayesian skipped for EHB0019622, r = 0\n",
      "Running uncensoring with method Baseline on magazine B\n",
      "Running uncensoring with method N1 on magazine C\n",
      "Running uncensoring with method N2 on magazine C\n",
      "Running uncensoring with method N3 on magazine C\n",
      "Running uncensoring with method EM on magazine C\n",
      "Running uncensoring with method PD on magazine C\n",
      "Running uncensoring with method Nahmias on magazine C\n",
      "Running uncensoring with method Conrad on magazine C\n",
      "Running uncensoring with method Bayesian on magazine C\n",
      "Running uncensoring with method Baseline on magazine C\n",
      "Running uncensoring with method N1 on magazine D\n",
      "Running uncensoring with method N2 on magazine D\n",
      "Running uncensoring with method N3 on magazine D\n",
      "Running uncensoring with method EM on magazine D\n",
      "Running uncensoring with method PD on magazine D\n",
      "Running uncensoring with method Nahmias on magazine D\n",
      "Running uncensoring with method Conrad on magazine D\n",
      "Running uncensoring with method Bayesian on magazine D\n",
      "Running uncensoring with method Baseline on magazine D\n",
      "Running uncensoring with method N1 on magazine E\n",
      "Running uncensoring with method N2 on magazine E\n",
      "Running uncensoring with method N3 on magazine E\n",
      "Running uncensoring with method EM on magazine E\n",
      "Running uncensoring with method PD on magazine E\n",
      "Running uncensoring with method Nahmias on magazine E\n",
      "Running uncensoring with method Conrad on magazine E\n",
      "Running uncensoring with method Bayesian on magazine E\n",
      "Running uncensoring with method Baseline on magazine E\n",
      "Running uncensoring with method N1 on magazine F\n",
      "Running uncensoring with method N2 on magazine F\n",
      "Running uncensoring with method N3 on magazine F\n",
      "Running uncensoring with method EM on magazine F\n",
      "Running uncensoring with method PD on magazine F\n",
      "Running uncensoring with method Nahmias on magazine F\n",
      "Running uncensoring with method Conrad on magazine F\n",
      "Running uncensoring with method Bayesian on magazine F\n",
      "Running uncensoring with method Baseline on magazine F\n",
      "Running uncensoring with method N1 on magazine G\n",
      "Running uncensoring with method N2 on magazine G\n",
      "Running uncensoring with method N3 on magazine G\n",
      "Running uncensoring with method EM on magazine G\n",
      "Running uncensoring with method PD on magazine G\n",
      "Running uncensoring with method Nahmias on magazine G\n",
      "Running uncensoring with method Conrad on magazine G\n",
      "Running uncensoring with method Bayesian on magazine G\n",
      "Running uncensoring with method Baseline on magazine G\n",
      "Running uncensoring with method N1 on magazine H\n",
      "Running uncensoring with method N2 on magazine H\n",
      "Running uncensoring with method N3 on magazine H\n",
      "Running uncensoring with method EM on magazine H\n",
      "Running uncensoring with method PD on magazine H\n",
      "Skipping POS EHB0019622: invalid lambda 0.0\n",
      "Running uncensoring with method Nahmias on magazine H\n",
      "Running uncensoring with method Conrad on magazine H\n",
      "Running uncensoring with method Bayesian on magazine H\n",
      "Bayesian skipped for EHB0019622, r = 0\n",
      "Running uncensoring with method Baseline on magazine H\n",
      "Running uncensoring with method N1 on magazine I\n",
      "Running uncensoring with method N2 on magazine I\n",
      "Running uncensoring with method N3 on magazine I\n",
      "Running uncensoring with method EM on magazine I\n",
      "Running uncensoring with method PD on magazine I\n",
      "Running uncensoring with method Nahmias on magazine I\n",
      "Running uncensoring with method Conrad on magazine I\n",
      "Running uncensoring with method Bayesian on magazine I\n",
      "Running uncensoring with method Baseline on magazine I\n"
     ]
    }
   ],
   "source": [
    "#demo, only use first few pos per magazine\n",
    "#skip 'Agrawal' for now, runs slowly and poor results\n",
    "uncensoring_methods = ['N1', 'N2', 'N3', 'EM', 'PD', 'Nahmias', 'Conrad','Bayesian', 'Baseline']\n",
    "original_data = []\n",
    "\n",
    "for letter in 'ABCDEFGHI':\n",
    "    df_magazine = pd.read_csv('original_data/'+letter+'_20250212_ZQ0.35_ZG0.4_testfile.csv')# adjust this accordingly\n",
    "    df_magazine['Magazine'] = letter\n",
    "    df_magazine = df_magazine[df_magazine[\"EHASTRA_EH_NUMMER\"].str.extract(r\"(\\d+)\", expand=False).astype(int) < 24000]\n",
    "    print(letter, df_magazine['EHASTRA_EH_NUMMER'].unique())\n",
    "    print(len(df_magazine))\n",
    "    original_data.append(df_magazine)\n",
    "\n",
    "df_results = apply_uncensoring(uncensoring_methods, original_data)\n",
    "df_results.to_csv('Uncensoring_results.csv', index=False)# adjust this accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b434d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncensoring_methods = ['N1', 'N2', 'N3', 'EM', 'PD', 'Nahmias', 'Conrad', 'Baseline', 'Bayesian', 'Agrawal']\n",
    "original_data = []\n",
    "\n",
    "for letter in 'ABCDEFGHI':\n",
    "    df_magazine = pd.read_csv('original_data/'+letter+'_20250212_ZQ0.35_ZG0.4_testfile.csv')# adjust this accordingly\n",
    "    df_magazine['Magazine'] = letter\n",
    "    original_data.append(df_magazine)\n",
    "\n",
    "df_results = apply_uncensoring(uncensoring_methods, original_data)\n",
    "df_results.to_csv('Uncensoring_results.csv', index=False)# adjust this accordingly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
