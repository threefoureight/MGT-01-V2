{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdfd6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1447e",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11222f",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a69a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_baseline_uncensoring(df):\n",
    "    df_result = df.copy()\n",
    "    df_result['Baseline_Demand'] = df_result['Verkauf']\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01287916",
   "metadata": {},
   "source": [
    "## Naive Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed537889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_n1_uncensoring(df):\n",
    "    \"\"\"\n",
    "    N1: replace censored values with mean of all values within each POS.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['N1_Demand'] = df['Verkauf'].copy()\n",
    "\n",
    "    # Identify closed observations\n",
    "    is_closed = (df[\"Zensiert\"] == 1)\n",
    "\n",
    "    # Compute mean Verkauf per group and broadcast using transform\n",
    "    group_means = df.groupby(['EHASTRA_EH_NUMMER'])['N1_Demand'].transform('mean').round()\n",
    "    \n",
    "    # Replace closed observations\n",
    "    df.loc[is_closed, 'N1_Demand'] = group_means[is_closed]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_n2_uncensoring(df):\n",
    "    \"\"\"\n",
    "    N2: replace censored values with mean of uncensored values within each POS.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['N2_Demand'] = df['Verkauf'].copy()\n",
    "    \n",
    "    # Calculate mean of uncensored observations for each group\n",
    "    uncensored_means = (\n",
    "        df[df['Zensiert'] == 0]\n",
    "        .groupby(['EHASTRA_EH_NUMMER'])['Verkauf']\n",
    "        .mean()\n",
    "        .rename('uncensored_mean')\n",
    "        .reset_index()\n",
    "        .round()\n",
    "    )\n",
    "    \n",
    "    # Merge back to original DataFrame\n",
    "    df = df.merge(uncensored_means, on=['EHASTRA_EH_NUMMER'], how='left')\n",
    "    \n",
    "    # CREATE MASK AFTER MERGE - this is the key fix\n",
    "    mask = (df['Zensiert'] == 1) & df['uncensored_mean'].notna()\n",
    "    df.loc[mask, 'N2_Demand'] = df.loc[mask, 'uncensored_mean']\n",
    "    \n",
    "    return df.drop('uncensored_mean', axis=1)\n",
    "\n",
    "\n",
    "def apply_n3_uncensoring(df):\n",
    "    \"\"\"\n",
    "    N3: replace censored values with max(current value, average of non-censored values) within each POS.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['N3_Demand'] = df['Verkauf'].copy()\n",
    "    \n",
    "    # Compute open group means and merge back\n",
    "    open_means = (\n",
    "        df[df['Zensiert'] == 0]\n",
    "        .groupby('EHASTRA_EH_NUMMER')['Verkauf']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'Verkauf': 'open_mean'})\n",
    "        .round()\n",
    "    )\n",
    "    \n",
    "    df = df.merge(open_means, on='EHASTRA_EH_NUMMER', how='left')\n",
    "    \n",
    "    # CREATE MASK AFTER MERGE - this is the key fix\n",
    "    mask = (df['Zensiert'] == 1) & df['open_mean'].notna()\n",
    "    df.loc[mask, 'N3_Demand'] = np.maximum(\n",
    "        df.loc[mask, 'Verkauf'],\n",
    "        df.loc[mask, 'open_mean']\n",
    "    )\n",
    "    \n",
    "    return df.drop('open_mean', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1efcfc",
   "metadata": {},
   "source": [
    "## EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773729fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "def apply_em_uncensoring(df, max_iter=30, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - max_iter: Maximum number of iterations\n",
    "    - tolerance: Convergence tolerance\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['EM_Demand'] = df['Verkauf'].copy()\n",
    "    \n",
    "    stockout_condition = (df['Zensiert'] == 1)\n",
    "    \n",
    "    for (pos,), group in df.groupby(['EHASTRA_EH_NUMMER']):\n",
    "        try:\n",
    "            group_stockout = stockout_condition.loc[group.index]\n",
    "            \n",
    "            if not group_stockout.any():\n",
    "                continue\n",
    "            \n",
    "            sales = group['EM_Demand'].values\n",
    "            is_stockout = group_stockout.values\n",
    "            \n",
    "            uncensored = sales[~is_stockout]\n",
    "            censored = sales[is_stockout]\n",
    "            \n",
    "            # Skip if no uncensored data - but keep original values\n",
    "            if len(uncensored) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Quick lambda initialization\n",
    "            lambda_est = np.mean(uncensored) if len(uncensored) > 0 else np.mean(sales) * 1.5\n",
    "            lambda_est = max(lambda_est, 0.1)\n",
    "            \n",
    "            # Fast EM loop\n",
    "            for iteration in range(max_iter):\n",
    "                lambda_old = lambda_est\n",
    "                \n",
    "                # Batch E-step\n",
    "                surv_prob = 1 - poisson.cdf(censored - 1, lambda_est)\n",
    "                exact_prob = poisson.pmf(censored, lambda_est)\n",
    "                surv_prob = np.maximum(surv_prob, 1e-12)\n",
    "                \n",
    "                expected = lambda_est + censored * exact_prob / surv_prob\n",
    "                expected = np.maximum(expected, censored.astype(float))\n",
    "                \n",
    "                # M-step\n",
    "                lambda_est = max(np.mean(np.concatenate([uncensored, expected])), 0.1)\n",
    "                \n",
    "                if abs(lambda_est - lambda_old) < tolerance:\n",
    "                    break\n",
    "            \n",
    "            # Final update\n",
    "            surv_prob = 1 - poisson.cdf(censored - 1, lambda_est)\n",
    "            exact_prob = poisson.pmf(censored, lambda_est)\n",
    "            surv_prob = np.maximum(surv_prob, 1e-12)\n",
    "            \n",
    "            final_expected = lambda_est + censored * exact_prob / surv_prob\n",
    "            final_expected = np.maximum(final_expected, censored.astype(float))\n",
    "            \n",
    "            # Update original dataframe\n",
    "            stockout_indices = group.index[is_stockout]\n",
    "            df.loc[stockout_indices, 'EM_Demand'] = final_expected.round()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"EM error for POS {pos}: {e}\")\n",
    "            # Fill with original Verkauf values for this POS when error occurs\n",
    "            group_stockout_indices = group.index[stockout_condition.loc[group.index]]\n",
    "            df.loc[group_stockout_indices, 'EM_Demand'] = df.loc[group_stockout_indices, 'Verkauf']\n",
    "            continue\n",
    "    \n",
    "    # Replace any NaN values with original Verkauf\n",
    "    nan_mask = df['EM_Demand'].isna()\n",
    "    df.loc[nan_mask, 'EM_Demand'] = df.loc[nan_mask, 'Verkauf']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228eb84a",
   "metadata": {},
   "source": [
    "## PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a25eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "def apply_pd_uncensoring(df, tau=0.5, max_iter=20, tolerance=1e-4):\n",
    "    \"\"\"PD with skip for invalid POS groups and fallback to original Verkauf\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"is_closed\"] = (df[\"Zensiert\"] == 1)\n",
    "    df['PD_Demand'] = df['Verkauf'].copy().astype(float)\n",
    "\n",
    "    def compute_pd_projection1(obs_val, lambda_est, tau):\n",
    "        \"\"\"Your PD projection with NaN protection\"\"\"\n",
    "        try:\n",
    "            # Check for NaN inputs\n",
    "            if pd.isna(obs_val) or pd.isna(lambda_est) or lambda_est <= 0:\n",
    "                return float(obs_val) if not pd.isna(obs_val) else 0.0\n",
    "            \n",
    "            obs_val = int(round(obs_val))\n",
    "            \n",
    "            def objective(k_proj):\n",
    "                k_proj = int(round(k_proj))\n",
    "                \n",
    "                if k_proj < obs_val:\n",
    "                    return float('inf')\n",
    "                \n",
    "                # Area A: P(obs_val <= X <= k_proj)\n",
    "                area_A = poisson.cdf(k_proj, lambda_est) - poisson.cdf(obs_val - 1, lambda_est)\n",
    "                \n",
    "                # Area B: P(X > k_proj)\n",
    "                area_B = 1 - poisson.cdf(k_proj, lambda_est)\n",
    "                \n",
    "                if area_B > 1e-10:\n",
    "                    ratio = area_A / area_B\n",
    "                    target_ratio = (1 - tau) / tau\n",
    "                    return abs(ratio - target_ratio)\n",
    "                else:\n",
    "                    return abs(area_A - (1 - tau))\n",
    "            \n",
    "            # Search for optimal projection\n",
    "            upper_bound = int(obs_val + max(10, int(3 * np.sqrt(lambda_est))))\n",
    "            \n",
    "            best_k = obs_val\n",
    "            best_objective = float('inf')\n",
    "            \n",
    "            for k in range(int(obs_val), upper_bound + 1):\n",
    "                obj_val = objective(k)\n",
    "                if obj_val < best_objective:\n",
    "                    best_objective = obj_val\n",
    "                    best_k = k\n",
    "            \n",
    "            return float(best_k)\n",
    "            \n",
    "        except Exception:\n",
    "            # Fallback to original value\n",
    "            return float(obs_val) if not pd.isna(obs_val) else 0.0\n",
    "    \n",
    "    # Process groups\n",
    "    grouped = df.groupby('EHASTRA_EH_NUMMER')\n",
    "    \n",
    "    for pos, group in grouped:\n",
    "        try:\n",
    "            open_mask = ~group['is_closed']\n",
    "            closed_mask = group['is_closed']\n",
    "            \n",
    "            open_sales = group.loc[open_mask, 'PD_Demand'].values\n",
    "            closed_sales = group.loc[closed_mask, 'PD_Demand'].values\n",
    "            \n",
    "            # SKIP POS WITHOUT VALID UNCENSORED DATA\n",
    "            if len(open_sales) == 0 or np.all(pd.isna(open_sales)):\n",
    "                # print(f\"Skipping POS {pos}: no valid uncensored data\")\n",
    "                continue\n",
    "            \n",
    "            # Initialize lambda parameter\n",
    "            lambda_est = np.mean(open_sales[~pd.isna(open_sales)])\n",
    "            \n",
    "            # SKIP IF LAMBDA IS INVALID\n",
    "            if pd.isna(lambda_est) or lambda_est <= 0:\n",
    "                print(f\"Skipping POS {pos}: invalid lambda {lambda_est}\")\n",
    "                continue\n",
    "            \n",
    "            lambda_est = max(lambda_est, 0.1)\n",
    "            closed_indices = group[closed_mask].index.values\n",
    "            \n",
    "            # Iterative process\n",
    "            for iteration in range(max_iter):\n",
    "                lambda_old = lambda_est\n",
    "                \n",
    "                # Project closed observations\n",
    "                projected_values = np.array([\n",
    "                    compute_pd_projection1(obs, lambda_est, tau)\n",
    "                    for obs in closed_sales\n",
    "                ])\n",
    "                \n",
    "                # Re-estimate lambda\n",
    "                all_values = np.concatenate([open_sales, projected_values])\n",
    "                lambda_est = np.mean(all_values)\n",
    "                lambda_est = max(lambda_est, 0.1)\n",
    "                \n",
    "                # Check convergence\n",
    "                if abs(lambda_est - lambda_old) < tolerance:\n",
    "                    break\n",
    "            \n",
    "            # Final projection\n",
    "            final_projections = np.array([\n",
    "                compute_pd_projection1(obs, lambda_est, tau)\n",
    "                for obs in closed_sales\n",
    "            ])\n",
    "            \n",
    "            # Update dataframe\n",
    "            df.loc[closed_indices, 'PD_Demand'] = final_projections.round()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"PD error for POS {pos}: {e}\")\n",
    "            # FALLBACK: Keep original values for this POS\n",
    "            continue\n",
    "    \n",
    "    # FINAL FALLBACK: Any remaining NaN values get original Verkauf\n",
    "    nan_mask = df['PD_Demand'].isna()\n",
    "    df.loc[nan_mask, 'PD_Demand'] = df.loc[nan_mask, 'Verkauf']\n",
    "    \n",
    "    return df.drop('is_closed', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2141346",
   "metadata": {},
   "source": [
    "## Conrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b85a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "def berechnung(links, rechts, n, N, r, x_summe, value_tol=0.00001, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Till's Code\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        mu = (links + rechts) / 2\n",
    "        wert_0 = (x_summe - mu * n) * (1 - poisson.cdf(N-1, mu)) + mu * (n - r) * (1 - poisson.cdf(N-2, mu))\n",
    "        \n",
    "        if iteration < 3:\n",
    "            print(f\"Iter {iteration}: mu={mu:.4f}, wert_0={wert_0:.8f}\")\n",
    "        \n",
    "        if abs(wert_0) < value_tol:\n",
    "            print(f\"Converged after {iteration} iterations: mu={mu:.4f}, wert_0={wert_0:.8f}\")\n",
    "            return mu\n",
    "        elif wert_0 < 0:  \n",
    "            rechts = mu\n",
    "        elif wert_0 > 0:\n",
    "            links = mu\n",
    "            \n",
    "        iteration += 1\n",
    "    \n",
    "    print(f\"✗ Max iterations reached: mu={mu:.4f}\")\n",
    "    return mu\n",
    "\n",
    "def test_conrad_example():\n",
    "    links = 1\n",
    "    rechts = 100\n",
    "    n = 13\n",
    "    N = 10\n",
    "    r = 7\n",
    "    x_summe = 58\n",
    "    \n",
    "    print(f\"links={links}, rechts={rechts}\")\n",
    "    print(f\"n={n}, N={N}, r={r}, x_summe={x_summe}\")\n",
    "    \n",
    "    result = berechnung(links, rechts, n, N, r, x_summe)\n",
    "    print(f\"Result: μ = {result:.4f}\")\n",
    "    print(f\"Expected: μ ≈ 10.18\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_order_specific_mu_dict(df):\n",
    "\n",
    "    order_specific_mu_dict = {}\n",
    "    \n",
    "    for (year, week), week_data in df.groupby(['Heftjahr', 'Heftnummer']):\n",
    "        for bezug_val, group in week_data.groupby('Bezug'):\n",
    "            n = len(group)\n",
    "            N = bezug_val\n",
    "            \n",
    "            # Count non-stockouts\n",
    "            stockouts_mask = (group['Zensiert'] == 1)\n",
    "            r = n - stockouts_mask.sum()  # r = number of NON-stockouts\n",
    "            \n",
    "            # x_summe = sum of UNCENSORED observations only\n",
    "            uncensored_sales = group[~stockouts_mask]['Verkauf']\n",
    "            x_summe = uncensored_sales.sum()\n",
    "            \n",
    "            # Skip problematic cases\n",
    "            if n < 3:\n",
    "                continue\n",
    "            if r == n:  # No stockouts = no censoring information\n",
    "                continue\n",
    "            if r == 0:  # All stockouts = no uncensored observations\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                links = 1\n",
    "                rechts = 100\n",
    "                mu_est = berechnung(links, rechts, n, N, r, x_summe)\n",
    "                if mu_est:\n",
    "                    key = (year, week, bezug_val)\n",
    "                    order_specific_mu_dict[key] = mu_est\n",
    "            except Exception as e:\n",
    "                print(f\"Error in week {week}, Bezug {N}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Successfully estimated μ for {len(order_specific_mu_dict)} groups\")\n",
    "    return order_specific_mu_dict\n",
    "\n",
    "def expected_poisson_tail(mu, N, max_k=200):\n",
    "    \"\"\"\n",
    "    Compute E[X | X >= N] for X ~ Poisson(mu)\n",
    "    \"\"\"\n",
    "    k_vals = np.arange(N, max_k)\n",
    "    pmf = poisson.pmf(k_vals, mu)\n",
    "    tail_prob = 1 - poisson.cdf(N - 1, mu)\n",
    "    if tail_prob < 1e-8:\n",
    "        return N  # fallback: don't uncensor\n",
    "    return np.sum(k_vals * pmf) / tail_prob\n",
    "\n",
    "# Internal function\n",
    "def apply_conrad_uncensoring_1(df, order_specific_mu_dict):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with Verkauf, Bezug, is_stockout, Heftjahr, Heftnummer,\n",
    "    replace Verkauf with E[X | X >= Bezug] when censored.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['Conrad_Demand'] = df['Verkauf'].copy()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Skip if no stockout occurred\n",
    "        if row['Zensiert'] == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get the order-quantity-specific demand parameter\n",
    "        key = (row['Heftjahr'], row['Heftnummer'], row['Bezug'])\n",
    "        mu = order_specific_mu_dict.get(key, None)\n",
    "\n",
    "        if mu is None:\n",
    "            # no estimate available for this specific (week, order_quantity) — keep original value\n",
    "            continue\n",
    "\n",
    "        # POS sold out — uncensor using the specific distribution for this order quantity\n",
    "        est_demand = expected_poisson_tail(mu, row['Bezug'])\n",
    "        df.at[idx, 'Conrad_Demand'] = np.round(est_demand)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Wrapper function for script\n",
    "def apply_conrad_uncensoring(df):\n",
    "    \"\"\"\n",
    "    WRAPPER FUNCTION: This is what gets called by the main processing loop\n",
    "    \"\"\"\n",
    "    # Step 1: Create mu dictionary from the dataset\n",
    "    order_specific_mu_dict = create_order_specific_mu_dict(df)\n",
    "    \n",
    "    # Step 2: Apply uncensoring using the mu dictionary\n",
    "    return apply_conrad_uncensoring_1(df, order_specific_mu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e33a2",
   "metadata": {},
   "source": [
    "## Nahmias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2cfdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def nahmias_estimation(sales, S):\n",
    "    \"\"\"\n",
    "    Nahmias method for censored normal data\n",
    "    \"\"\"\n",
    "    sales = np.array(sales)\n",
    "    n = len(sales)\n",
    "    \n",
    "    observed = sales[sales < S]\n",
    "    r = len(observed)\n",
    "    p = r / n\n",
    "    \n",
    "    if r < 2 or r >= n-1 or p <= 0 or p >= 1:\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        x_bar = np.mean(observed)\n",
    "        s2 = np.var(observed, ddof=1)\n",
    "        z = norm.ppf(p)\n",
    "        \n",
    "        sigma_hat2 = s2 / (1 - (z * norm.pdf(z) / p) - (norm.pdf(z)**2 / p**2))\n",
    "        sigma_hat = np.sqrt(sigma_hat2)\n",
    "        mu_hat = x_bar + sigma_hat * norm.pdf(z) / p\n",
    "        \n",
    "        if not np.isfinite(mu_hat) or not np.isfinite(sigma_hat) or sigma_hat <= 0:\n",
    "            return None, None\n",
    "            \n",
    "        return mu_hat, sigma_hat\n",
    "        \n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def create_order_specific_nahmias_dict(df):\n",
    "    \"\"\"\n",
    "    Create μ and σ estimates for each (week, order_quantity) combination\n",
    "    \"\"\"\n",
    "    order_specific_mu_dict = {}\n",
    "    order_specific_sigma_dict = {}\n",
    "\n",
    "    for (year, week), week_data in df.groupby(['Heftjahr', 'Heftnummer']):\n",
    "        for bezug_val, group in week_data.groupby('Bezug'):\n",
    "            n = len(group)\n",
    "            S = bezug_val\n",
    "            \n",
    "            if n < 5:\n",
    "                continue\n",
    "            \n",
    "            sales = group['Verkauf'].values\n",
    "            \n",
    "            try:\n",
    "                mu_est, sigma_est = nahmias_estimation(sales, S)\n",
    "                if mu_est is not None and sigma_est is not None:\n",
    "                    key = (year, week, bezug_val)\n",
    "                    order_specific_mu_dict[key] = mu_est\n",
    "                    order_specific_sigma_dict[key] = sigma_est\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    # print(f\"Successfully estimated μ,σ for {len(order_specific_mu_dict)} groups\")\n",
    "    return order_specific_mu_dict, order_specific_sigma_dict\n",
    "\n",
    "def expected_normal_tail(mu, sigma, S):\n",
    "    \"\"\"\n",
    "    Compute E[X | X >= S] for X ~ Normal(mu, sigma)\n",
    "    \"\"\"\n",
    "    if sigma <= 0:\n",
    "        return S\n",
    "    \n",
    "    z = (S - mu) / sigma\n",
    "    \n",
    "    if z > 6:\n",
    "        return S\n",
    "    \n",
    "    tail_prob = 1 - norm.cdf(z)\n",
    "    \n",
    "    if tail_prob < 1e-10:\n",
    "        return S\n",
    "    \n",
    "    expected_value = mu + sigma * norm.pdf(z) / tail_prob\n",
    "    \n",
    "    return expected_value\n",
    "\n",
    "def apply_nahmias_uncensoring_1(df, order_specific_mu_dict, order_specific_sigma_dict):\n",
    "    \"\"\"\n",
    "    Uncensor dataset using Nahmias estimates\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['Nahmias_Demand'] = df['Verkauf'].copy()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['Zensiert'] == 0:\n",
    "            continue\n",
    "            \n",
    "        key = (row['Heftjahr'], row['Heftnummer'], row['Bezug'])\n",
    "        mu = order_specific_mu_dict.get(key, None)\n",
    "        sigma = order_specific_sigma_dict.get(key, None)\n",
    "\n",
    "        if mu is None or sigma is None:\n",
    "            continue\n",
    "\n",
    "        est_demand = expected_normal_tail(mu, sigma, row['Bezug'])\n",
    "        df.at[idx, 'Nahmias_Demand'] = np.round(est_demand)\n",
    "\n",
    "    return df\n",
    "\n",
    "def test_nahmias():\n",
    "    \"\"\"Test implementation\"\"\"\n",
    "    mu_true = 100\n",
    "    sigma_true = 30\n",
    "    S = 110\n",
    "    n = 100\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    demand = np.random.normal(mu_true, sigma_true, n)\n",
    "    sales = np.minimum(demand, S)\n",
    "    \n",
    "    print(\"Testing Nahmias implementation:\")\n",
    "    print(f\"True μ: {mu_true}, True σ: {sigma_true}\")\n",
    "    print(f\"S (censoring limit): {S}\")\n",
    "    print(f\"Sample size: {n}\")\n",
    "    \n",
    "    mu_hat, sigma_hat = nahmias_estimation(sales, S)\n",
    "    \n",
    "    naive_mean = np.mean(sales)\n",
    "    naive_std = np.std(sales, ddof=1)\n",
    "    \n",
    "    print(f\"True mean: {mu_true}\")\n",
    "    print(f\"Naive mean (sales): {naive_mean:.2f}\")\n",
    "    print(f\"Corrected estimator (Nahmias): {mu_hat:.2f}\")\n",
    "    print(f\"True Std.Dev.: {sigma_true}\")\n",
    "    print(f\"Corrected Std.Dev.: {sigma_hat:.2f}\")\n",
    "    print(f\"Naive Std.Dev. (sales): {naive_std:.2f}\")\n",
    "    \n",
    "    return mu_hat, sigma_hat\n",
    "\n",
    "def apply_nahmias_uncensoring(df):\n",
    "    \"\"\"\n",
    "    WRAPPER FUNCTION: This is what gets called by the main processing loop\n",
    "    \"\"\"\n",
    "    # Step 1: Create mu dictionary from the dataset\n",
    "    order_specific_mu_dict, compute_mu_sigma_nahmias = create_order_specific_nahmias_dict(df)\n",
    "    \n",
    "    # Step 2: Apply uncensoring using the mu dictionary\n",
    "    return apply_nahmias_uncensoring_1(df, order_specific_mu_dict, compute_mu_sigma_nahmias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a4066",
   "metadata": {},
   "source": [
    "## Agrawal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b3e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy.stats import nbinom\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def find_p(N, bezug, F_observed):\n",
    "    \"\"\"\"\n",
    "    Find p for a given N such that P(X < S) from the Negative Binomial\n",
    "    matches the empirically observed uncensored probability.\n",
    "    \"\"\"\n",
    "    func = lambda p: abs(nbinom.cdf(bezug - 1, N, p) - F_observed)\n",
    "    res = minimize_scalar(func, bounds=(0.01, 0.99), method='bounded')\n",
    "    return res.x\n",
    "\n",
    "\n",
    "def uncensor(N_hat, p_hat, s, max):\n",
    "    '''\n",
    "    Compute E[X|X>=s] for X~NB(N_hat,p_hat).\n",
    "    max: upper demand limit used in calculation\n",
    "    '''\n",
    "    X_vals = np.arange(s, max)\n",
    "    pmf = nbinom.pmf(X_vals, N_hat, p_hat)\n",
    "    tail_prob = 1- nbinom.cdf(s-1, N_hat, p_hat)\n",
    "    if tail_prob < 1e-8:\n",
    "        return s  # fallback: don't uncensor\n",
    "    exp = np.sum(X_vals * pmf) / tail_prob\n",
    "    return int(np.round(exp))\n",
    "\n",
    "\n",
    "def compute_NB_result(group_key_data):\n",
    "    '''\n",
    "    Given one hierarchical group, calculate the Agrawal demand\n",
    "    '''\n",
    "    def objective(N, bezug, x_bar, F_observed):\n",
    "        p = find_p(N, bezug, F_observed)\n",
    "        prob_uncensored = nbinom.cdf(bezug - 1, N, p)\n",
    "        if prob_uncensored == 0.0: return 10**8 #if probability of demand being uncensored is too low, return a high objective value\n",
    "        expect = nbinom.expect(lambda x: x, args=(N, p), lb=0, ub=bezug-1)\n",
    "        mean_uncensored = expect / prob_uncensored\n",
    "        return abs(mean_uncensored - x_bar)\n",
    "    \n",
    "\n",
    "    (bezug, period), group = group_key_data\n",
    "    group = group.dropna(subset=['Bezug', 'Verkauf'])\n",
    "\n",
    "    group_uncensored = group[group['Zensiert']==0]\n",
    "    x_bar = group_uncensored['Verkauf'].mean()\n",
    "    F_observed = len(group_uncensored)/len(group)\n",
    "\n",
    "    result = minimize_scalar(lambda N: objective(N,bezug,x_bar,F_observed), bounds=(0.1, 500), method='bounded')\n",
    "    N_hat = result.x\n",
    "    p_hat = find_p(N_hat, bezug, F_observed)\n",
    "\n",
    "    ub = max(100, bezug*2)\n",
    "    demand = uncensor(N_hat, p_hat, bezug, max = ub)\n",
    "    print(f'Period: {period}, Bezug: {bezug}, N: {N_hat}, p: {p_hat}')\n",
    "    return {'Period': period, 'Bezug': bezug, 'Agrawal_Demand': demand}\n",
    "\n",
    "\n",
    "def apply_agrawal_uncensoring(dataframe, max_cpus=None):\n",
    "    '''\n",
    "    Group magazine data by bezug and period run Agrawal algorithm in parallel.\n",
    "    Returns original dataframe with new columns Agrawal_Demand\n",
    "    '''\n",
    "    #Step 1: calculate Agrawal demand\n",
    "    df = dataframe.copy()\n",
    "    df.sort_values(['Bezug', 'Period'], inplace=True)\n",
    "    grouped = list(df.groupby(['Bezug', 'Period']))\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_cpus) as executor:\n",
    "        futures = [executor.submit(compute_NB_result, group) for group in grouped]\n",
    "        for future in as_completed(futures):\n",
    "            results.append(future.result())\n",
    "    df_results= pd.DataFrame(results)\n",
    "\n",
    "    #Step 2: calculate hierarchical group uncensored fraction)\n",
    "    group_data = []\n",
    "    for (bezug,period),group in df.groupby(['Bezug', 'Period']):\n",
    "        group_data.append({'Bezug':bezug, 'Period':period, 'Uncensored_fraction': len(group[group['Zensiert']==0])/len(group)})\n",
    "    df_group = pd.DataFrame(group_data)\n",
    "    df_results = pd.merge(df_results, df_group, how='outer', on=['Bezug','Period'])\n",
    "\n",
    "    #Step 3: add results to original data\n",
    "    df_join = pd.merge(df, df_results, how=\"outer\", on=['Bezug','Period'])\n",
    "\n",
    "    #only use Agrawal estimation for censored data (use real sales value for not censored data)\n",
    "    df_join['Agrawal_Demand'] = np.where(df_join['Zensiert'], df_join['Agrawal_Demand'], df_join['Verkauf'])\n",
    "\n",
    "    #only use Agrawal estimation for valid results (uncensored fraction is not 0 or 1)\n",
    "    df_join['Agrawal_Demand'] = np.where((df_join['Uncensored_fraction']==0.0)|(df_join['Uncensored_fraction']==1.0), df_join['Verkauf'], df_join['Agrawal_Demand'])\n",
    "    \n",
    "    df_join.drop(['Uncensored_fraction'],axis='columns', inplace=True)\n",
    "    return df_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fa76bc",
   "metadata": {},
   "source": [
    "## Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf7974fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy.stats import nbinom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86b176",
   "metadata": {},
   "source": [
    "### Precomputations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b1eec",
   "metadata": {},
   "source": [
    "To improve runtime of Bayesian method, precompute PMF and CMF of negative binomial distribution. Also precompute conditional sales probabilities.<br>\n",
    "note: $p_{idx}$ corresponds to the index in the grid $0.900, 0.901, ..., 0.990$\n",
    "<br><br>\n",
    "pmf_table[$n,r,p_{idx}$] = nbinom.pmf($n,r,p$) where $p = 0.900+p_{idx}*0.001$<br>\n",
    "cmf_table[$n,r,p_idx$] = nbinom.cmf($n,r,p$) where $p = 0.900+p_{idx}*0.001$<br>\n",
    "cond_pmf_sales[$X, s, r, p_{idx}$] = P($X$ sales with $s$ inventory and demand ~ NB($r,p$)) where $p = 0.900+p_{idx}*0.001$<br>\n",
    "<br>\n",
    "We choose to precompute above values up to $n=200, r=300, X=200, s=200$ as these upper bounds will cover most data points. For remaining cases not included in the tables, we directly compute them.\n",
    "<br>\n",
    "If you do not want to use precomputations, simply set the max_n and max_r variables to $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18627e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 301, 91)\n",
      "(201, 301, 91)\n",
      "(201, 201, 301, 91)\n"
     ]
    }
   ],
   "source": [
    "#upper bounds for initial NB precomputing\n",
    "max_n = 200\n",
    "max_r = 300\n",
    "\n",
    "#non-parametric p distribution values for NB fitting\n",
    "p_grid = np.linspace(0.900, 0.990, 91)\n",
    "\n",
    "def precompute_NB ():\n",
    "    '''\n",
    "    Calculates tables containing NB pmf and cmf values up to given bounds.\n",
    "    Output: two tables with shape (max_n, max_r, len(p_grid))\n",
    "    '''\n",
    "    pmf_table = np.zeros((max_n+1, max_r +1, len(p_grid)))\n",
    "    cmf_table = np.zeros((max_n+1, max_r +1, len(p_grid)))\n",
    "    for p_idx, p in enumerate(p_grid):\n",
    "        for r in range(max_r+1):\n",
    "            pmf_table[:, r, p_idx] = nbinom.pmf(np.arange(0,max_n+1), r, p)\n",
    "            cmf_table[:, r, p_idx] = nbinom.cdf(np.arange(0,max_n+1), r, p)\n",
    "    np.save(\"pmf_table.npy\", pmf_table)\n",
    "    np.save(\"cmf_table.npy\", cmf_table)\n",
    "\n",
    "def precompute_cond_pmf_sale():\n",
    "    '''\n",
    "    Calculates table containing conditional pmf sales (given demand~NB(r,p) and inventory is s)\n",
    "    For uncensored sales (X < s): table returns nbinom.pmf(X, r, p)\n",
    "    For censored sales (X >= s): table returns 1 - nbinom.cmf(s-1, r, p)\n",
    "\n",
    "    Output: one table with shape (max_n, max_n, max_r, len(p_grid))\n",
    "    Specifically, value at index (X, s, r, p) represents probability of getting X sales given demand~NB(r,p) and inventory is s\n",
    "    '''\n",
    "    cond_pmf_sale = np.zeros((max_n+1, max_n+1, max_r+1, len(p_grid)))\n",
    "\n",
    "    for p_idx, p in enumerate(p_grid):\n",
    "        for X in range(max_n+1):\n",
    "            for s in range(max_n+1):\n",
    "                for r in range(max_r+1):\n",
    "                    cond_pmf_sale[X, s, r, p_idx] = pmf_table[X, r, p_idx] if X < s else 1 - cmf_table[s-1, r, p_idx]\n",
    "    np.save(\"cond_pmf_sale_table.npy\", cond_pmf_sale)\n",
    "\n",
    "precompute_NB()\n",
    "pmf_table = np.load('pmf_table.npy')\n",
    "cmf_table = np.load('cmf_table.npy')\n",
    "precompute_cond_pmf_sale()\n",
    "cond_pmf_sales_table = np.load('cond_pmf_sale_table.npy')\n",
    "\n",
    "print(pmf_table.shape) # (201,301,91)\n",
    "print(cmf_table.shape) # (201,301,91)\n",
    "print(cond_pmf_sales_table.shape) # (201,201,301,91)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db276c2f",
   "metadata": {},
   "source": [
    "### Bayesian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e456dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_quantity(r, c_ratio, pmf_p, lo=0, hi=100):\n",
    "    '''\n",
    "    Returns minimum order quantity such that predicted demand is above the critical ratio\n",
    "    '''\n",
    "    # Use binary search to determine lowest value such that quantile > critical ratio\n",
    "    # quantile at mid = ∑_(demand = 0 to mid)∑_(p_grid)  pmf(demand, r, p)*π(p)\n",
    "    while lo < hi:\n",
    "        mid = (lo + hi) // 2\n",
    "        \n",
    "        if mid > max_n or r > max_r:\n",
    "            quantile =  np.sum([np.dot(nbinom.pmf(y, r, p_grid), pmf_p) for y in range(int(mid) + 1)])\n",
    "        else:\n",
    "            quantile = np.sum([np.dot(pmf_table[y, r, :], pmf_p) for y in range(int(mid) + 1)])\n",
    "\n",
    "        if quantile < c_ratio:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            hi = mid\n",
    "    return lo\n",
    "\n",
    "def uncensor(r, pmf_p, s, max):\n",
    "    '''\n",
    "    Calculate E[X|X>=s, X~NB(r, p)] with p distribution = pmf_p\n",
    "    max: upper demand limit used in calculation\n",
    "    '''\n",
    "    # tail_prob = probability of demand >=s = ∑_(p_grid) sf(s, r, p)*π(p)\n",
    "    # pmf = pmf of demand s to max (entry for demand x = ∑_(p_grid) pmf(x, r, p)*π(p))\n",
    "    if r > max_r or max > max_n:\n",
    "        pmf = nbinom.pmf(np.arange(s,max+1)[:, None], r, np.array(p_grid)) @ pmf_p.T #shape = (max+1-s,1)\n",
    "        tail_prob = np.dot(1 - nbinom.cdf(s-1, r, p_grid)[None, :], pmf_p.T).item()\n",
    "    else:\n",
    "        pmf = np.dot(pmf_table[s:max+1, r,:], pmf_p)\n",
    "        tail_prob = np.dot(1 - cmf_table[s-1, r,:], pmf_p)\n",
    "\n",
    "    if tail_prob < 1e-8:\n",
    "        return s  # fallback: don't uncensor (demand>=s is unlikely)\n",
    "    \n",
    "    exp = np.dot(np.arange(s,max+1), pmf)/tail_prob\n",
    "    return int(np.round(exp))\n",
    "\n",
    "def compute_bayesian_result(group_key_data, c_ratio):\n",
    "    '''\n",
    "    given one pos group and critical ratio, calculate the bayesian demand and order quantity\n",
    "    if there is no 2022-2023 data or if mean 2022-2023 sales is too low (<0.052), this pos is skipped\n",
    "    '''\n",
    "    (pos,), group = group_key_data\n",
    "    group = group.dropna(subset=['Bezug', 'Verkauf'])\n",
    "    results = []\n",
    "\n",
    "    #determine appropriate r value using 2022-2023 mean sales\n",
    "    train_data = group[group['Heftjahr']<2024]\n",
    "    if len(train_data) == 0:\n",
    "        print(f'skipped, {pos} has no 2022-2023 data points')\n",
    "        return\n",
    "    else:\n",
    "        mean_2223 = train_data['Verkauf'].mean()\n",
    "        r = int(mean_2223*0.95/0.05)+1 #NB(r, 0.95) has mean ≈ mean(2022-2023 sales)\n",
    "        if r == 0.0:\n",
    "            print(f'skipped, {pos} has r = 0')\n",
    "            return #don't use Bayesian for this POS\n",
    "\n",
    "    # initialize distribution of p, π, to Uniform\n",
    "    pmf_p = np.full(len(p_grid), 1.0 / len(p_grid))\n",
    "\n",
    "    #update π with each new verkauf-bezug data point\n",
    "    for row in group.itertuples():\n",
    "        #cond_sale_probs = probability of sale given demand~NB(r,p) and inventory level\n",
    "        if row.Bezug > max_n or r > max_r:\n",
    "            cond_sale_probs = [nbinom.pmf(row.Verkauf, r, p) if row.Verkauf < row.Bezug else 1-nbinom.cdf(row.Bezug - 1, r, p) for p in p_grid] #shape = len(p_grid)\n",
    "        else:\n",
    "            cond_sale_probs = cond_pmf_sales_table[int(row.Verkauf), int(row.Bezug), r, :]  #shape = len(p_grid)\n",
    "            \n",
    "        sale_prob = np.dot(cond_sale_probs, pmf_p) #summed over all p values\n",
    "\n",
    "        #do not update π if probability of sale is too low\n",
    "        if sale_prob != 0.0:\n",
    "            pmf_p *= cond_sale_probs / sale_prob\n",
    "        \n",
    "        ub = max(50,2*int(train_data['Verkauf'].max()))\n",
    "        Q = quantile_quantity(r, c_ratio, pmf_p, lo=0, hi=ub)\n",
    "        if Q == ub: print(f'{pos}, {row.Period}, reached max Q of {Q}') #check if upper bound is limiting Q output\n",
    "\n",
    "        if row.Zensiert == 1: demand = uncensor(r, pmf_p, int(row.Bezug), max= ub)\n",
    "        else: demand = row.Verkauf\n",
    "\n",
    "        results.append({'EHASTRA_EH_NUMMER': pos, 'Period':row.Period, 'Bayesian_Demand': demand, 'Bayesian_Q': Q})\n",
    "\n",
    "    print(f'{pos}, r = {r}, final pmf_p: {pmf_p}')\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def apply_bayesian_uncensoring_optimization(dataframe, c_ratio, max_cpus = None):\n",
    "    '''\n",
    "    Group magazine data by pos and run bayesian algorithm in parallel.\n",
    "    Returns original dataframe with new columns Bayesian_Demand, and Bayesian_Q\n",
    "    '''\n",
    "    #Step 1: calculate Bayesian Demand and Q values\n",
    "    df = dataframe.copy()\n",
    "    df.sort_values(['EHASTRA_EH_NUMMER', 'Period'], inplace=True)\n",
    "    grouped = df.groupby(['EHASTRA_EH_NUMMER'])\n",
    "    grouped = list(grouped)\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_cpus) as executor:\n",
    "        futures  = [executor.submit(compute_bayesian_result, group, c_ratio) for group in grouped]\n",
    "        for future in as_completed(futures):\n",
    "            if future.result() is not None: results.append(future.result())\n",
    "    df_Bayesian = pd.concat(results)\n",
    "\n",
    "    #Step 2: add results to original dataframe\n",
    "    df_join = pd.merge(df, df_Bayesian, how=\"outer\", on=['EHASTRA_EH_NUMMER', 'Period'])\n",
    "    df_join.set_index(['EHASTRA_EH_NUMMER', 'Period'])\n",
    "\n",
    "    # Step 3: For POS where Bayesian does not work, fill in missing demand predictions with verkauf. Missing Q values are not changed\n",
    "    df_join.loc[df_join['Verkauf'].notna() & df_join['Bayesian_Demand'].isna(), 'Bayesian_Demand'] = df_join['Verkauf']\n",
    "    return df_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670aa0a3",
   "metadata": {},
   "source": [
    "# Run Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ['EHA0017186' 'EHC0004006' 'EHC0005070' 'EHH0010681' 'EHB0023505'\n",
      " 'EHB0019844' 'EHE0003391' 'EHA0021360']\n",
      "B ['EHA0020265' 'EHD0015991' 'EHB0019622' 'EHG0029342' 'EHB0023505'\n",
      " 'EHG0003391']\n",
      "C ['EHG0029342' 'EHB0023505']\n",
      "D ['EHA0021360' 'EHB0019844' 'EHH0010681' 'EHI0025292']\n",
      "E ['EHB0019622' 'EHB0024268' 'EHC0005070' 'EHE0003391' 'EHI0025292']\n",
      "F ['EHG0006538']\n",
      "G ['EHB0023505' 'EHG0006538']\n",
      "H ['EHB0019622' 'EHB0023505' 'EHG0006538' 'EHH0011851' 'EHI0022887']\n",
      "I ['EHA0020265' 'EHH0010681' 'EHH0011851' 'EHI0025292']\n",
      "Finished uncensoring with method N1 on magazine A\n",
      "Finished uncensoring with method N1 on magazine B\n",
      "Finished uncensoring with method N1 on magazine C\n",
      "Finished uncensoring with method N1 on magazine D\n",
      "Finished uncensoring with method N1 on magazine E\n",
      "Finished uncensoring with method N1 on magazine F\n",
      "Finished uncensoring with method N1 on magazine G\n",
      "Finished uncensoring with method N1 on magazine H\n",
      "Finished uncensoring with method N1 on magazine I\n",
      "Finished uncensoring with method N2 on magazine A\n",
      "Finished uncensoring with method N2 on magazine B\n",
      "Finished uncensoring with method N2 on magazine C\n",
      "Finished uncensoring with method N2 on magazine D\n",
      "Finished uncensoring with method N2 on magazine E\n",
      "Finished uncensoring with method N2 on magazine F\n",
      "Finished uncensoring with method N2 on magazine G\n",
      "Finished uncensoring with method N2 on magazine H\n",
      "Finished uncensoring with method N2 on magazine I\n",
      "Finished uncensoring with method N3 on magazine A\n",
      "Finished uncensoring with method N3 on magazine B\n",
      "Finished uncensoring with method N3 on magazine C\n",
      "Finished uncensoring with method N3 on magazine D\n",
      "Finished uncensoring with method N3 on magazine E\n",
      "Finished uncensoring with method N3 on magazine F\n",
      "Finished uncensoring with method N3 on magazine G\n",
      "Finished uncensoring with method N3 on magazine H\n",
      "Finished uncensoring with method N3 on magazine I\n"
     ]
    }
   ],
   "source": [
    "def apply_uncensoring(methods, dataframes):\n",
    "    '''\n",
    "    methods: list of uncensoring methods to use\n",
    "    dataframes: list of original data (magazines) to use\n",
    "    Returns one dataframe containing uncensoring results across all methods and dataframes, along with original columns\n",
    "    '''\n",
    "    method_functions = {\n",
    "        'N1': apply_n1_uncensoring,\n",
    "        'N2': apply_n2_uncensoring,\n",
    "        'N3': apply_n3_uncensoring,\n",
    "        'EM': apply_em_uncensoring,\n",
    "        'PD': apply_pd_uncensoring,\n",
    "        'Nahmias': apply_nahmias_uncensoring,\n",
    "        'Conrad': apply_conrad_uncensoring,\n",
    "        'Baseline': apply_baseline_uncensoring,\n",
    "        'Bayesian': apply_bayesian_uncensoring_optimization,\n",
    "        'Agrawal': apply_agrawal_uncensoring\n",
    "    }\n",
    "    df_results = None\n",
    "    for method in methods:\n",
    "        df_method = pd.DataFrame()\n",
    "        for dataframe in dataframes:\n",
    "            df = method_functions[method](dataframe)\n",
    "            print(f'Finished uncensoring with method {method} on magazine {dataframe['Magazine'].unique()[0]}')\n",
    "            df_method = pd.concat([df_method, df], ignore_index = True)\n",
    "        if df_results is None: df_results = df_method #for the first method\n",
    "        else: df_results = pd.merge(df_results, df_method, how='outer', on = ['Magazine','EHASTRA_EH_NUMMER'])\n",
    "    return df_results\n",
    "\n",
    "#demo, only use first few pos per magazine\n",
    "\n",
    "uncensoring_methods = ['N1', 'N2', 'N3', 'EM', 'PD', 'Nahmias', 'Conrad', 'Baseline', 'Bayesian', 'Agrawal']\n",
    "original_data = []\n",
    "\n",
    "for letter in 'ABCDEFGHI':\n",
    "    df_magazine = pd.read_csv('original_data/'+letter+'_20250212_ZQ0.35_ZG0.4_testfile.csv')# adjust this accordingly\n",
    "    df_magazine['Magazine'] = letter\n",
    "    df_magazine = df_magazine[df_magazine[\"EHASTRA_EH_NUMMER\"].str.extract(r\"(\\d+)\", expand=False).astype(int) < 30000]\n",
    "    print(letter, df_magazine['EHASTRA_EH_NUMMER'].unique())\n",
    "    original_data.append(df_magazine)\n",
    "\n",
    "df_results = apply_uncensoring(uncensoring_methods, original_data)\n",
    "df_results.to_csv('Uncensoring_results.csv', index=False)# adjust this accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b434d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m original_data = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m letter \u001b[38;5;129;01min\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mABCDEFGHI\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     df_magazine = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moriginal_data/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43mletter\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_20250212_ZQ0.35_ZG0.4_testfile.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# adjust this accordingly\u001b[39;00m\n\u001b[32m      6\u001b[39m     df_magazine[\u001b[33m'\u001b[39m\u001b[33mMagazine\u001b[39m\u001b[33m'\u001b[39m] = letter\n\u001b[32m      7\u001b[39m     original_data.append(df_magazine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joyir\\Downloads\\MIT\\MISTI\\TUM PREP\\MGT01 Code\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joyir\\Downloads\\MIT\\MISTI\\TUM PREP\\MGT01 Code\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joyir\\Downloads\\MIT\\MISTI\\TUM PREP\\MGT01 Code\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1965\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1966\u001b[39m         new_col_dict = col_dict\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m     df = \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m     \u001b[38;5;28mself\u001b[39m._currow += new_rows\n\u001b[32m   1976\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joyir\\Downloads\\MIT\\MISTI\\TUM PREP\\MGT01 Code\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joyir\\Downloads\\MIT\\MISTI\\TUM PREP\\MGT01 Code\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joyir\\Downloads\\MIT\\MISTI\\TUM PREP\\MGT01 Code\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    149\u001b[39m axes = [columns, index]\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joyir\\Downloads\\MIT\\MISTI\\TUM PREP\\MGT01 Code\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2139\u001b[39m, in \u001b[36mcreate_block_manager_from_column_arrays\u001b[39m\u001b[34m(arrays, axes, consolidate, refs)\u001b[39m\n\u001b[32m   2121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[32m   2122\u001b[39m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[32m   2123\u001b[39m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2135\u001b[39m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[32m   2136\u001b[39m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[32m   2138\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2139\u001b[39m         blocks = \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2140\u001b[39m         mgr = BlockManager(blocks, axes, verify_integrity=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2141\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joyir\\Downloads\\MIT\\MISTI\\TUM PREP\\MGT01 Code\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2212\u001b[39m, in \u001b[36m_form_blocks\u001b[39m\u001b[34m(arrays, consolidate, refs)\u001b[39m\n\u001b[32m   2209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype.type, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[32m   2210\u001b[39m     dtype = np.dtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m values, placement = \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[32m   2214\u001b[39m     values = ensure_wrapped_if_datetimelike(values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joyir\\Downloads\\MIT\\MISTI\\TUM PREP\\MGT01 Code\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2252\u001b[39m, in \u001b[36m_stack_arrays\u001b[39m\u001b[34m(tuples, dtype)\u001b[39m\n\u001b[32m   2249\u001b[39m first = arrays[\u001b[32m0\u001b[39m]\n\u001b[32m   2250\u001b[39m shape = (\u001b[38;5;28mlen\u001b[39m(arrays),) + first.shape\n\u001b[32m-> \u001b[39m\u001b[32m2252\u001b[39m stacked = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[32m   2254\u001b[39m     stacked[i] = arr\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "uncensoring_methods = ['N1', 'N2', 'N3', 'EM', 'PD', 'Nahmias', 'Conrad', 'Baseline', 'Bayesian', 'Agrawal']\n",
    "original_data = []\n",
    "\n",
    "for letter in 'ABCDEFGHI':\n",
    "    df_magazine = pd.read_csv('original_data/'+letter+'_20250212_ZQ0.35_ZG0.4_testfile.csv')# adjust this accordingly\n",
    "    df_magazine['Magazine'] = letter\n",
    "    original_data.append(df_magazine)\n",
    "\n",
    "df_results = apply_uncensoring(uncensoring_methods, original_data)\n",
    "df_results.to_csv('Uncensoring_results.csv', index=False)# adjust this accordingly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
